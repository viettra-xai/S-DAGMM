{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, plot_confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "from dagmm_INSE_6180 import DAGMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\")\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "#     if tight_layout:\n",
    "#         plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiement 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = './datasets/chiller/df_dataset/' # path to the dataset\n",
    "\n",
    "def load_data(load_path, filename):\n",
    "    csv_load_path = os.path.join(load_path, filename)\n",
    "    return pd.read_csv(csv_load_path)\n",
    "\n",
    "data_1 = load_data(load_path, 'chiller10.csv') # chiller data of severity level 1\n",
    "\n",
    "# split data so that 20% data will be used to create outliers\n",
    "data_11, data_12 = train_test_split(data_1, test_size=0.2, random_state=0) \n",
    "X_12, y_12  = data_12.iloc[:, :-1], data_12.iloc[:, -1] \n",
    "columns = X_12.columns\n",
    "\n",
    "# generate artificial outliers\n",
    "choices = np.random.choice(X_12.size, 2*X_12.shape[0], replace=False)\n",
    "X_12_flatted = X_12.to_numpy().ravel()\n",
    "X_12_flatted[choices] = 500\n",
    "X_12 = pd.DataFrame(X_12_flatted.reshape(X_12.shape))\n",
    "X_12.columns = columns\n",
    "data_12 = X_12.copy()\n",
    "data_12['label'] = np.asarray(y_12, dtype=int)\n",
    "data = data_11.append(data_12)\n",
    "data = data.drop('Unnamed: 0', axis='columns')\n",
    "data_trn, data_tst = train_test_split(data, test_size=0.5, random_state=0) # Final training data and test data for experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset 1 - Origninal data (Using data with outliers)\n",
    "\n",
    "X_trn, y_trn = data_trn.iloc[:, :-1], data_trn.iloc[:, -1]\n",
    "X_tst, y_tst = data_tst.iloc[:, :-1], data_tst.iloc[:, -1]\n",
    "# Scaling data\n",
    "scaler_d1 = StandardScaler().fit(X_trn)\n",
    "X_trn_scaled, X_tst_scaled = scaler_d1.transform(X_trn), scaler_d1.transform(X_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\travi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2584/2584 [==============================] - mean_loss: 65.5875 - val_loss: 73.4050\n",
      "Best Epoch: 1\n",
      "Epoch 2/5\n",
      "2584/2584 [==============================] - mean_loss: 54.5781 - val_loss: 71.1902\n",
      "Best Epoch: 2\n",
      "Epoch 3/5\n",
      "2584/2584 [==============================] - mean_loss: 55.4264 - val_loss: 69.3696\n",
      "Best Epoch: 3\n",
      "Epoch 4/5\n",
      "2584/2584 [==============================] - mean_loss: 57.8381 - val_loss: 68.4148\n",
      "Best Epoch: 4\n",
      "Epoch 5/5\n",
      "2584/2584 [==============================] - mean_loss: 50.2026 - val_loss: 66.2742\n",
      "Best Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\travi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2571/2571 [==============================] - mean_loss: 67.3976 - val_loss: 60.9765\n",
      "Best Epoch: 1\n",
      "Epoch 2/5\n",
      "2571/2571 [==============================] - mean_loss: 55.4612 - val_loss: 58.9029\n",
      "Best Epoch: 2\n",
      "Epoch 3/5\n",
      "2571/2571 [==============================] - mean_loss: 53.9959 - val_loss: 59.6007\n",
      "Best Epoch: 2\n",
      "Epoch 4/5\n",
      "2571/2571 [==============================] - mean_loss: 48.8315 - val_loss: 59.2720\n",
      "Best Epoch: 2\n",
      "Epoch 5/5\n",
      "2571/2571 [==============================] - mean_loss: 48.3568 - val_loss: 56.3186\n",
      "Best Epoch: 5\n",
      "Epoch 1/5\n",
      "  32/2568 [>.............................] - mean_loss: 215.5642"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\travi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2568/2568 [==============================] - mean_loss: 68.3850 - val_loss: 46.1159\n",
      "Best Epoch: 1\n",
      "Epoch 2/5\n",
      "2568/2568 [==============================] - mean_loss: 63.9982 - val_loss: 45.2757\n",
      "Best Epoch: 2\n",
      "Epoch 3/5\n",
      "2568/2568 [==============================] - mean_loss: 55.3658 - val_loss: 44.1078\n",
      "Best Epoch: 3\n",
      "Epoch 4/5\n",
      "2568/2568 [==============================] - mean_loss: 53.9028 - val_loss: 43.3835\n",
      "Best Epoch: 4\n",
      "Epoch 5/5\n",
      "2568/2568 [==============================] - mean_loss: 50.3095 - val_loss: 44.5296\n",
      "Best Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\travi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2607/2607 [==============================] - mean_loss: 72.3541 - val_loss: 54.7270\n",
      "Best Epoch: 1\n",
      "Epoch 2/5\n",
      "2607/2607 [==============================] - mean_loss: 59.7844 - val_loss: 53.3832\n",
      "Best Epoch: 2\n",
      "Epoch 3/5\n",
      "2607/2607 [==============================] - mean_loss: 56.0234 - val_loss: 54.0885\n",
      "Best Epoch: 2\n",
      "Epoch 4/5\n",
      "2607/2607 [==============================] - mean_loss: 57.2279 - val_loss: 51.6586\n",
      "Best Epoch: 4\n",
      "Epoch 5/5\n",
      "2607/2607 [==============================] - mean_loss: 54.6797 - val_loss: 50.6700\n",
      "Best Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\travi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2592/2592 [==============================] - mean_loss: 72.8435 - val_loss: 63.1094\n",
      "Best Epoch: 1\n",
      "Epoch 2/5\n",
      "2592/2592 [==============================] - mean_loss: 60.5728 - val_loss: 59.1093\n",
      "Best Epoch: 2\n",
      "Epoch 3/5\n",
      "2592/2592 [==============================] - mean_loss: 55.8571 - val_loss: 59.1069\n",
      "Best Epoch: 3\n",
      "Epoch 4/5\n",
      "2592/2592 [==============================] - mean_loss: 49.4678 - val_loss: 62.2973\n",
      "Best Epoch: 3\n",
      "Epoch 5/5\n",
      "2592/2592 [==============================] - mean_loss: 53.8584 - val_loss: 58.4131\n",
      "Best Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\travi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2616/2616 [==============================] - mean_loss: 64.6448 - val_loss: 52.8731\n",
      "Best Epoch: 1\n",
      "Epoch 2/5\n",
      "2616/2616 [==============================] - mean_loss: 60.1300 - val_loss: 52.4163\n",
      "Best Epoch: 2\n",
      "Epoch 3/5\n",
      "2616/2616 [==============================] - mean_loss: 57.9173 - val_loss: 52.3893\n",
      "Best Epoch: 3\n",
      "Epoch 4/5\n",
      "2616/2616 [==============================] - mean_loss: 57.7619 - val_loss: 51.0548\n",
      "Best Epoch: 4\n",
      "Epoch 5/5\n",
      "2616/2616 [==============================] - mean_loss: 51.6815 - val_loss: 49.0566\n",
      "Best Epoch: 5\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\travi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2635/2635 [==============================] - mean_loss: 74.0030 - val_loss: 61.5650\n",
      "Best Epoch: 1\n",
      "Epoch 2/5\n",
      "2635/2635 [==============================] - mean_loss: 87.8357 - val_loss: 61.6121\n",
      "Best Epoch: 1\n",
      "Epoch 3/5\n",
      "2635/2635 [==============================] - mean_loss: 53.0782 - val_loss: 58.9097\n",
      "Best Epoch: 3\n",
      "Epoch 4/5\n",
      "2635/2635 [==============================] - mean_loss: 52.0063 - val_loss: 57.7779\n",
      "Best Epoch: 4\n",
      "Epoch 5/5\n",
      "2635/2635 [==============================] - mean_loss: 54.8561 - val_loss: 56.6532\n",
      "Best Epoch: 5\n",
      "Epoch 1/5\n",
      "  32/2591 [>.............................] - mean_loss: 379.3294"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\travi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2591/2591 [==============================] - mean_loss: 73.0112 - val_loss: 61.2016\n",
      "Best Epoch: 1\n",
      "Epoch 2/5\n",
      "2591/2591 [==============================] - mean_loss: 59.4395 - val_loss: 59.4086\n",
      "Best Epoch: 2\n",
      "Epoch 3/5\n",
      "2591/2591 [==============================] - mean_loss: 52.4360 - val_loss: 58.7098\n",
      "Best Epoch: 3\n",
      "Epoch 4/5\n",
      "2591/2591 [==============================] - mean_loss: 54.9602 - val_loss: 57.6492\n",
      "Best Epoch: 4\n",
      "Epoch 5/5\n",
      "2591/2591 [==============================] - mean_loss: 53.3770 - val_loss: 58.8881\n",
      "Best Epoch: 4\n"
     ]
    }
   ],
   "source": [
    "# Dataset 2 - Cleaning data using S-DAGMM (Detecting and removing outliers)\n",
    "model_dagmms = []\n",
    "data_rate = 80 # consider 20% data is outliers\n",
    "data_trn_clean_d2 = pd.DataFrame()\n",
    "energy_thresholds = []\n",
    "\n",
    "# Offline phase\n",
    "n = len(data_trn['label'].unique()) \n",
    "\n",
    "# create and train 8 individual DAGMMs according to 8 different data labels (normal and seven different fault types)\n",
    "for i in range(n): \n",
    "    model_dagmm = DAGMM(comp_hiddens=[50, 30, 20, 10], comp_activation=\"elu\",\n",
    "                      est_hiddens=[15, 3], est_activation=\"elu\", est_dropout_ratio=0.2,\n",
    "                      n_epochs=5, batch_size=32, normalize=True)\n",
    "    data_trn_c, X_trn_c = data_trn[y_trn==i], X_trn[y_trn==i] # select the data according to the label\n",
    "    model_dagmm.build(X_trn_c)\n",
    "    model_dagmm.fit(X_trn_c)\n",
    "    energy = model_dagmm.predict(X_trn_c) # calculate the energy value\n",
    "    energy_threshold = np.percentile(energy, data_rate) # get the energy threshold to seperate inliers and outliers\n",
    "    data_trn_clean_d2 = data_trn_clean_d2.append(data_trn_c[energy <= energy_threshold]) # filter outliers\n",
    "    model_dagmms.append(model_dagmm) # save DAGMM models for online phase\n",
    "    energy_thresholds.append(energy_threshold) # save energy threshold for online phase\n",
    "\n",
    "X_trn_clean_d2, y_trn_clean_d2 = data_trn_clean_d2.iloc[:, :-1], data_trn_clean_d2.iloc[:, -1]\n",
    "\n",
    "# Online phase\n",
    "energies = []\n",
    "\n",
    "# Using trained DAGMMs to calculate the energy of testing samples\n",
    "\n",
    "for i in range(len(model_dagmms)):\n",
    "    energy = model_dagmms[i].predict(X_tst)\n",
    "    energies.append(energy)\n",
    "\n",
    "energies_np = np.asanyarray(energies)\n",
    "\n",
    "energy_thresholds_np = np.asarray(energy_thresholds).reshape(-1, 1) # Using energy_thresholds calculated in the offline phase\n",
    "votes = np.where(energies_np > energy_thresholds_np, 1, 0).T\n",
    "votes_sum = np.sum(votes, axis=1)\n",
    "\n",
    "# Using voting scheme to determine outliers (If five in eight individual DAGMMs determine\n",
    "# a sample as a outlier, S-DAGMM considers that sample as a outlier)\n",
    "idx_outlier = np.where(votes_sum >= 5) \n",
    "idx = np.where(votes_sum <= 5) # inlier data index\n",
    "data_tst_clean_d2 = data_tst.iloc[idx]\n",
    "X_tst_clean_d2, y_tst_clean_d2 = data_tst_clean_d2.iloc[:, :-1], data_tst_clean_d2.iloc[:, -1]\n",
    "\n",
    "# Scaling data (some base classifiers need scaled data for training)\n",
    "scaler_d2 = StandardScaler().fit(X_trn_clean_d2)\n",
    "X_trn_clean_scaled_d2, X_tst_clean_scaled_d2 = scaler_d2.transform(X_trn_clean_d2), scaler_d2.transform(X_tst_clean_d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2591, 65)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trn_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20764, 65), (20764, 65))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset 1 - Original dataset (Using outlier-contaminated data for training and testing classifiers)\n",
    "X_trn_, X_tst_ = X_trn, X_tst\n",
    "X_trn_scaled_, X_tst_scaled_ = X_trn_scaled, X_tst_scaled\n",
    "y_trn_, y_tst_ = y_trn, y_tst\n",
    "\n",
    "X_trn_.shape, X_tst_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN MODEL:\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_neighbors=9, p=1)\n",
      "Train Accuracy: 90.10787902138317 %\n",
      "Test Accuracy: 86.19244846850319 %\n",
      "====================================================================\n",
      "Confusion matrix: \n",
      " [[2277   37   41  108   57   54   18   15]\n",
      " [  93 2347   22   51   49   34    5   19]\n",
      " [ 135   22 2352   44   27   15   10   18]\n",
      " [ 136   39   43 2124  128   63   27   24]\n",
      " [  91   30   26  164 2150   93   16   29]\n",
      " [  81   30   38  152  114 2122   13   25]\n",
      " [  73   27   24   60   42   28 2243   59]\n",
      " [  67   24   27   60   65   33   42 2282]]\n",
      "====================================================================\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7711    0.8734    0.8191      2607\n",
      "           1     0.9182    0.8958    0.9069      2620\n",
      "           2     0.9141    0.8967    0.9053      2623\n",
      "           3     0.7687    0.8220    0.7945      2584\n",
      "           4     0.8169    0.8272    0.8220      2599\n",
      "           5     0.8690    0.8241    0.8459      2575\n",
      "           6     0.9448    0.8775    0.9099      2556\n",
      "           7     0.9235    0.8777    0.9000      2600\n",
      "\n",
      "    accuracy                         0.8619     20764\n",
      "   macro avg     0.8658    0.8618    0.8630     20764\n",
      "weighted avg     0.8658    0.8619    0.8630     20764\n",
      "\n",
      "====================================================================\n",
      "SVM MODEL:\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:  5.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=8, gamma=0.5)\n",
      "Train Accuracy: 99.79291080716625 %\n",
      "Test Accuracy: 84.56944712001541 %\n",
      "====================================================================\n",
      "Confusion matrix: \n",
      " [[2421    9   16   58   35   39   13   16]\n",
      " [ 331 2151   12   19   14   48   19   26]\n",
      " [ 277   10 2242   16   17   26   16   19]\n",
      " [ 306    7   20 2129   32   39   26   25]\n",
      " [ 306    8   15   43 2142   50   12   23]\n",
      " [ 298    6   12   35   50 2144   15   15]\n",
      " [ 301    3   15   19   15   27 2147   29]\n",
      " [ 281    7   16   20   31   31   30 2184]]\n",
      "====================================================================\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5355    0.9287    0.6793      2607\n",
      "           1     0.9773    0.8210    0.8923      2620\n",
      "           2     0.9549    0.8547    0.9020      2623\n",
      "           3     0.9102    0.8239    0.8649      2584\n",
      "           4     0.9170    0.8242    0.8681      2599\n",
      "           5     0.8918    0.8326    0.8612      2575\n",
      "           6     0.9425    0.8400    0.8883      2556\n",
      "           7     0.9345    0.8400    0.8847      2600\n",
      "\n",
      "    accuracy                         0.8457     20764\n",
      "   macro avg     0.8830    0.8456    0.8551     20764\n",
      "weighted avg     0.8829    0.8457    0.8551     20764\n",
      "\n",
      "====================================================================\n",
      "RANDOM FOREST MODEL:\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   49.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   49.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(n_estimators=550)\n",
      "Train Accuracy: 100.0 %\n",
      "Test Accuracy: 99.81699094586784 %\n",
      "====================================================================\n",
      "Confusion matrix: \n",
      " [[2596    0    0    1    7    1    2    0]\n",
      " [   0 2619    0    0    1    0    0    0]\n",
      " [   0    0 2622    0    1    0    0    0]\n",
      " [   3    0    0 2567   13    1    0    0]\n",
      " [   1    0    0    3 2593    2    0    0]\n",
      " [   0    0    0    0    1 2573    0    1]\n",
      " [   0    0    0    0    0    0 2556    0]\n",
      " [   0    0    0    0    0    0    0 2600]]\n",
      "====================================================================\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9985    0.9958    0.9971      2607\n",
      "           1     1.0000    0.9996    0.9998      2620\n",
      "           2     1.0000    0.9996    0.9998      2623\n",
      "           3     0.9984    0.9934    0.9959      2584\n",
      "           4     0.9912    0.9977    0.9944      2599\n",
      "           5     0.9984    0.9992    0.9988      2575\n",
      "           6     0.9992    1.0000    0.9996      2556\n",
      "           7     0.9996    1.0000    0.9998      2600\n",
      "\n",
      "    accuracy                         0.9982     20764\n",
      "   macro avg     0.9982    0.9982    0.9982     20764\n",
      "weighted avg     0.9982    0.9982    0.9982     20764\n",
      "\n",
      "====================================================================\n",
      "ADABOOST MODEL:\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  27 | elapsed:   37.1s remaining:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed:   42.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(learning_rate=0.1, n_estimators=200)\n",
      "Train Accuracy: 65.56058562897323 %\n",
      "Test Accuracy: 65.52205740705065 %\n",
      "====================================================================\n",
      "Confusion matrix: \n",
      " [[  69    0    0 2210  304   18    4    2]\n",
      " [   0 2608    0    4    4    1    0    3]\n",
      " [   0    0 2614    0    2    7    0    0]\n",
      " [   4    0    0  797 1754   19    1    9]\n",
      " [   0    0    0   38 2528   17    0   16]\n",
      " [   9    0    0  888 1655   13    0   10]\n",
      " [   1    0    0    9   29    3 2467   47]\n",
      " [   0    0    0    0   83    1    7 2509]]\n",
      "====================================================================\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8313    0.0265    0.0513      2607\n",
      "           1     1.0000    0.9954    0.9977      2620\n",
      "           2     1.0000    0.9966    0.9983      2623\n",
      "           3     0.2020    0.3084    0.2441      2584\n",
      "           4     0.3975    0.9727    0.5644      2599\n",
      "           5     0.1646    0.0050    0.0098      2575\n",
      "           6     0.9952    0.9652    0.9799      2556\n",
      "           7     0.9665    0.9650    0.9657      2600\n",
      "\n",
      "    accuracy                         0.6552     20764\n",
      "   macro avg     0.6946    0.6544    0.6014     20764\n",
      "weighted avg     0.6957    0.6552    0.6022     20764\n",
      "\n",
      "====================================================================\n"
     ]
    }
   ],
   "source": [
    "## 1. KNN MODEL\n",
    "\n",
    "print(\"KNN MODEL:\")\n",
    "\n",
    "k = [i for i in range(2,10)]\n",
    "p = [j for j in range(1,3)]\n",
    "param_grid = [{'n_neighbors': k, 'p': p}]\n",
    "knn_grid_search = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid, cv=3,\n",
    "                           scoring='f1_weighted', n_jobs=-1, return_train_score=True,\n",
    "                           verbose=1)\n",
    "\n",
    "knn_grid_search.fit(X_trn_scaled_, y_trn_)\n",
    "knn_clf = knn_grid_search.best_estimator_\n",
    "print(knn_clf)\n",
    "\n",
    "print(\"Train Accuracy:\", 100*knn_clf.score(X_trn_scaled_, y_trn_), chr(37))\n",
    "print(\"Test Accuracy:\", 100*knn_clf.score(X_tst_scaled_, y_tst_), chr(37))\n",
    "\n",
    "y_tst_pred_ = knn_clf.predict(X_tst_scaled_)\n",
    "print(\"====================================================================\")\n",
    "print(\"Confusion matrix: \\n\", confusion_matrix(y_tst_, y_tst_pred_))\n",
    "print(\"====================================================================\")\n",
    "print(\"Classification report: \\n\", classification_report(y_tst_, y_tst_pred_, digits=4))\n",
    "print(\"====================================================================\")\n",
    "\n",
    "\n",
    "## 2. SVM MODEL\n",
    "\n",
    "print(\"SVM MODEL:\")\n",
    "\n",
    "C = [2**i for i in range(0, 4)]\n",
    "gamma = [2**j for j in range(-2,2)]\n",
    "param_grid = [{'C': C, 'gamma': gamma}]\n",
    "svm_grid_search = GridSearchCV(SVC(), param_grid=param_grid, cv=3,\n",
    "                           scoring='f1_weighted', n_jobs=-1, return_train_score=True,\n",
    "                           verbose=1)\n",
    "svm_grid_search.fit(X_trn_scaled_, y_trn_)\n",
    "svm_clf = svm_grid_search.best_estimator_\n",
    "print(svm_clf)\n",
    "\n",
    "print(\"Train Accuracy:\", 100*svm_clf.score(X_trn_scaled_, y_trn_), chr(37))\n",
    "print(\"Test Accuracy:\", 100*svm_clf.score(X_tst_scaled_, y_tst_), chr(37))\n",
    "\n",
    "y_tst_pred_ = svm_clf.predict(X_tst_scaled_)\n",
    "print(\"====================================================================\")\n",
    "print(\"Confusion matrix: \\n\", confusion_matrix(y_tst_, y_tst_pred_))\n",
    "print(\"====================================================================\")\n",
    "print(\"Classification report: \\n\", classification_report(y_tst_, y_tst_pred_, digits=4))\n",
    "print(\"====================================================================\")\n",
    "\n",
    "## 3. RANDOM FOREST MODEL\n",
    "\n",
    "print(\"RANDOM FOREST MODEL:\")\n",
    "\n",
    "n_estimators_ = [int(x) for x in np.linspace(100, 550, 10)]\n",
    "\n",
    "param_grid = {'n_estimators':n_estimators_}\n",
    "rf_grid_search = GridSearchCV(RandomForestClassifier(), param_grid=param_grid, cv=3,\n",
    "                              scoring='f1_weighted', n_jobs=-1, return_train_score=True,\n",
    "                              verbose=1)\n",
    "rf_grid_search.fit(X_trn_, y_trn_)\n",
    "\n",
    "rf_clf = rf_grid_search.best_estimator_\n",
    "\n",
    "print(rf_clf)\n",
    "\n",
    "print(\"Train Accuracy:\", 100*rf_clf.score(X_trn_, y_trn_), chr(37))\n",
    "print(\"Test Accuracy:\", 100*rf_clf.score(X_tst_, y_tst_), chr(37))\n",
    "\n",
    "y_tst_pred_ = rf_clf.predict(X_tst_)\n",
    "print(\"====================================================================\")\n",
    "print(\"Confusion matrix: \\n\", confusion_matrix(y_tst_, y_tst_pred_))\n",
    "print(\"====================================================================\")\n",
    "print(\"Classification report: \\n\", classification_report(y_tst_, y_tst_pred_, digits=4))\n",
    "print(\"====================================================================\")\n",
    "\n",
    "\n",
    "# ## 4. LOGISTIC REGRESSION MODEL\n",
    "\n",
    "# C= np.logspace(-4,4,9)\n",
    "# solver=['newton-cg']\n",
    "\n",
    "# param_grid = [{'solver': solver, 'C': C}]\n",
    "# lr_grid_search = GridSearchCV(LogisticRegression(max_iter=100),\n",
    "#                            param_grid=param_grid, cv=3, scoring='f1_weighted',\n",
    "#                            n_jobs=-1, return_train_score=True, verbose=1)\n",
    "# lr_grid_search.fit(X_trn_scaled_, y_trn_)\n",
    "# lr_clf = lr_grid_search.best_estimator_\n",
    "# print(lr_clf)\n",
    "\n",
    "# print(\"Train Accuracy:\", 100*lr_clf.score(X_trn_scaled_, y_trn_), chr(37))\n",
    "# print(\"Test Accuracy:\", 100*lr_clf.score(X_tst_scaled_, y_tst_), chr(37))\n",
    "\n",
    "## 5. ADABOOST MODEL\n",
    "\n",
    "print(\"ADABOOST MODEL:\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators' : [100, 200, 300],\n",
    "    'learning_rate' : [0.001, 0.01, 0.1]\n",
    "}\n",
    "ad_grid_search = GridSearchCV(AdaBoostClassifier(), param_grid = param_grid,\n",
    "                                cv=3, scoring='f1_weighted', n_jobs=-1, return_train_score=True, verbose=1\n",
    "                                )\n",
    "ad_grid_search.fit(X_trn_, y_trn_)\n",
    "\n",
    "ad_clf = ad_grid_search.best_estimator_\n",
    "\n",
    "print(ad_clf)\n",
    "\n",
    "print(\"Train Accuracy:\", 100*ad_clf.score(X_trn_, y_trn_), chr(37))\n",
    "print(\"Test Accuracy:\", 100*ad_clf.score(X_tst_, y_tst_), chr(37))\n",
    "\n",
    "y_tst_pred_ = ad_clf.predict(X_tst_)\n",
    "print(\"====================================================================\")\n",
    "print(\"Confusion matrix: \\n\", confusion_matrix(y_tst_, y_tst_pred_))\n",
    "print(\"====================================================================\")\n",
    "print(\"Classification report: \\n\", classification_report(y_tst_, y_tst_pred_, digits=4))\n",
    "print(\"====================================================================\")\n",
    "\n",
    "\n",
    "# ## 6. GAUSIAN NAIVE BAYSESSIAN\n",
    "\n",
    "# params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "# gnb_grid_search = GridSearchCV(GaussianNB(), param_grid=params_NB, cv=3, scoring='accuracy',return_train_score=True)\n",
    "# gnb_grid_search.fit(X_trn_scaled_, y_trn_)\n",
    "\n",
    "# gnb_clf = gnb_grid_search.best_estimator_\n",
    "\n",
    "# print(gnb_clf)\n",
    "\n",
    "# print(\"Train Accuracy:\", 100*gnb_clf.score(X_trn_scaled_, y_trn_), chr(37))\n",
    "# print(\"Test Accuracy:\", 100*gnb_clf.score(X_tst_scaled_, y_tst_), chr(37))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 2 - Cleaning data using S-DAGMM (Using data cleaned by S-DAGMM to train and test classifiers)\n",
    "X_trn_, X_tst_ = X_trn_clean_d2, X_tst_clean_d2\n",
    "X_trn_scaled_, X_tst_scaled_ = X_trn_clean_scaled_d2, X_tst_clean_scaled_d2\n",
    "y_trn_, y_tst_ = y_trn_clean_d2, y_tst_clean_d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN MODEL:\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_neighbors=3, p=1)\n",
      "Train Accuracy: 99.40397350993378 %\n",
      "Test Accuracy: 97.1746677840297 %\n",
      "====================================================================\n",
      "Confusion matrix: \n",
      " [[2054    4   22   22    4    7    0    0]\n",
      " [  21 1882    0    4    1    0    0    0]\n",
      " [  91    1 2058    6    1    0    0    0]\n",
      " [  40    1    1 2028   32   13    0    0]\n",
      " [  10    0    1   46 2023   24    0    0]\n",
      " [  18    0    0   31   22 2037    0    0]\n",
      " [  13    1    2   11    2    7 2034    1]\n",
      " [   3    0    1    1    3    0    4 2118]]\n",
      "====================================================================\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9129    0.9721    0.9416      2113\n",
      "           1     0.9963    0.9864    0.9913      1908\n",
      "           2     0.9871    0.9541    0.9703      2157\n",
      "           3     0.9437    0.9589    0.9512      2115\n",
      "           4     0.9689    0.9615    0.9652      2104\n",
      "           5     0.9756    0.9663    0.9709      2108\n",
      "           6     0.9980    0.9821    0.9900      2071\n",
      "           7     0.9995    0.9944    0.9969      2130\n",
      "\n",
      "    accuracy                         0.9717     16706\n",
      "   macro avg     0.9727    0.9720    0.9722     16706\n",
      "weighted avg     0.9725    0.9717    0.9719     16706\n",
      "\n",
      "====================================================================\n",
      "SVM MODEL:\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=8, gamma=0.25)\n",
      "Train Accuracy: 99.98795906080674 %\n",
      "Test Accuracy: 98.55740452532025 %\n",
      "====================================================================\n",
      "Confusion matrix: \n",
      " [[2068    0   12   22    2    8    1    0]\n",
      " [   1 1891   16    0    0    0    0    0]\n",
      " [   0    0 2157    0    0    0    0    0]\n",
      " [  11    0    8 2068   13   13    2    0]\n",
      " [   6    0   13   21 2055    8    1    0]\n",
      " [   2    0    7    8   16 2075    0    0]\n",
      " [   1    0   28    0    0    0 2035    7]\n",
      " [   0    0    9    0    2    0    3 2116]]\n",
      "====================================================================\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9899    0.9787    0.9843      2113\n",
      "           1     1.0000    0.9911    0.9955      1908\n",
      "           2     0.9587    1.0000    0.9789      2157\n",
      "           3     0.9759    0.9778    0.9769      2115\n",
      "           4     0.9842    0.9767    0.9804      2104\n",
      "           5     0.9862    0.9843    0.9853      2108\n",
      "           6     0.9966    0.9826    0.9895      2071\n",
      "           7     0.9967    0.9934    0.9951      2130\n",
      "\n",
      "    accuracy                         0.9856     16706\n",
      "   macro avg     0.9860    0.9856    0.9857     16706\n",
      "weighted avg     0.9858    0.9856    0.9856     16706\n",
      "\n",
      "====================================================================\n",
      "RANDOM FOREST MODEL:\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   37.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   37.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(n_estimators=250)\n",
      "Train Accuracy: 100.0 %\n",
      "Test Accuracy: 99.86831078654376 %\n",
      "====================================================================\n",
      "Confusion matrix: \n",
      " [[2100    0    0   11    0    2    0    0]\n",
      " [   0 1908    0    0    0    0    0    0]\n",
      " [   0    0 2157    0    0    0    0    0]\n",
      " [   3    0    0 2110    0    2    0    0]\n",
      " [   0    0    0    0 2100    4    0    0]\n",
      " [   0    0    0    0    0 2108    0    0]\n",
      " [   0    0    0    0    0    0 2071    0]\n",
      " [   0    0    0    0    0    0    0 2130]]\n",
      "====================================================================\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9986    0.9938    0.9962      2113\n",
      "           1     1.0000    1.0000    1.0000      1908\n",
      "           2     1.0000    1.0000    1.0000      2157\n",
      "           3     0.9948    0.9976    0.9962      2115\n",
      "           4     1.0000    0.9981    0.9990      2104\n",
      "           5     0.9962    1.0000    0.9981      2108\n",
      "           6     1.0000    1.0000    1.0000      2071\n",
      "           7     1.0000    1.0000    1.0000      2130\n",
      "\n",
      "    accuracy                         0.9987     16706\n",
      "   macro avg     0.9987    0.9987    0.9987     16706\n",
      "weighted avg     0.9987    0.9987    0.9987     16706\n",
      "\n",
      "====================================================================\n",
      "ADABOOST MODEL:\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  27 | elapsed:   29.6s remaining:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed:   33.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(learning_rate=0.01, n_estimators=300)\n",
      "Train Accuracy: 67.13425647200482 %\n",
      "Test Accuracy: 66.97593678917754 %\n",
      "====================================================================\n",
      "Confusion matrix: \n",
      " [[ 580    0    0 1334  198    1    0    0]\n",
      " [   0 1908    0    0    0    0    0    0]\n",
      " [   0    0 2157    0    0    0    0    0]\n",
      " [ 109    0    0 1173  830    2    0    1]\n",
      " [   1    0    0  102 1987    2    0   12]\n",
      " [   4    0    0 1056 1044    4    0    0]\n",
      " [   1    0    0    0    0    1 1305  764]\n",
      " [   6    0    0    0   49    0    0 2075]]\n",
      "====================================================================\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8274    0.2745    0.4122      2113\n",
      "           1     1.0000    1.0000    1.0000      1908\n",
      "           2     1.0000    1.0000    1.0000      2157\n",
      "           3     0.3201    0.5546    0.4059      2115\n",
      "           4     0.4837    0.9444    0.6397      2104\n",
      "           5     0.4000    0.0019    0.0038      2108\n",
      "           6     1.0000    0.6301    0.7731      2071\n",
      "           7     0.7276    0.9742    0.8330      2130\n",
      "\n",
      "    accuracy                         0.6698     16706\n",
      "   macro avg     0.7198    0.6725    0.6335     16706\n",
      "weighted avg     0.7166    0.6698    0.6299     16706\n",
      "\n",
      "====================================================================\n"
     ]
    }
   ],
   "source": [
    "## 1. KNN MODEL\n",
    "\n",
    "print(\"KNN MODEL:\")\n",
    "\n",
    "k = [i for i in range(2,10)]\n",
    "p = [j for j in range(1,3)]\n",
    "param_grid = [{'n_neighbors': k, 'p': p}]\n",
    "knn_grid_search = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid, cv=3,\n",
    "                           scoring='f1_weighted', n_jobs=-1, return_train_score=True,\n",
    "                           verbose=1)\n",
    "\n",
    "knn_grid_search.fit(X_trn_scaled_, y_trn_)\n",
    "knn_clf = knn_grid_search.best_estimator_\n",
    "print(knn_clf)\n",
    "\n",
    "print(\"Train Accuracy:\", 100*knn_clf.score(X_trn_scaled_, y_trn_), chr(37))\n",
    "print(\"Test Accuracy:\", 100*knn_clf.score(X_tst_scaled_, y_tst_), chr(37))\n",
    "\n",
    "y_tst_pred_ = knn_clf.predict(X_tst_scaled_)\n",
    "print(\"====================================================================\")\n",
    "print(\"Confusion matrix: \\n\", confusion_matrix(y_tst_, y_tst_pred_))\n",
    "print(\"====================================================================\")\n",
    "print(\"Classification report: \\n\", classification_report(y_tst_, y_tst_pred_, digits=4))\n",
    "print(\"====================================================================\")\n",
    "\n",
    "\n",
    "## 2. SVM MODEL\n",
    "\n",
    "print(\"SVM MODEL:\")\n",
    "\n",
    "C = [2**i for i in range(0, 4)]\n",
    "gamma = [2**j for j in range(-2,2)]\n",
    "param_grid = [{'C': C, 'gamma': gamma}]\n",
    "svm_grid_search = GridSearchCV(SVC(), param_grid=param_grid, cv=3,\n",
    "                           scoring='f1_weighted', n_jobs=-1, return_train_score=True,\n",
    "                           verbose=1)\n",
    "svm_grid_search.fit(X_trn_scaled_, y_trn_)\n",
    "svm_clf = svm_grid_search.best_estimator_\n",
    "print(svm_clf)\n",
    "\n",
    "print(\"Train Accuracy:\", 100*svm_clf.score(X_trn_scaled_, y_trn_), chr(37))\n",
    "print(\"Test Accuracy:\", 100*svm_clf.score(X_tst_scaled_, y_tst_), chr(37))\n",
    "\n",
    "y_tst_pred_ = svm_clf.predict(X_tst_scaled_)\n",
    "print(\"====================================================================\")\n",
    "print(\"Confusion matrix: \\n\", confusion_matrix(y_tst_, y_tst_pred_))\n",
    "print(\"====================================================================\")\n",
    "print(\"Classification report: \\n\", classification_report(y_tst_, y_tst_pred_, digits=4))\n",
    "print(\"====================================================================\")\n",
    "\n",
    "## 3. RANDOM FOREST MODEL\n",
    "\n",
    "print(\"RANDOM FOREST MODEL:\")\n",
    "\n",
    "n_estimators_ = [int(x) for x in np.linspace(100, 550, 10)]\n",
    "\n",
    "param_grid = {'n_estimators':n_estimators_}\n",
    "rf_grid_search = GridSearchCV(RandomForestClassifier(), param_grid=param_grid, cv=3,\n",
    "                              scoring='f1_weighted', n_jobs=-1, return_train_score=True,\n",
    "                              verbose=1)\n",
    "rf_grid_search.fit(X_trn_, y_trn_)\n",
    "\n",
    "rf_clf = rf_grid_search.best_estimator_\n",
    "\n",
    "print(rf_clf)\n",
    "\n",
    "print(\"Train Accuracy:\", 100*rf_clf.score(X_trn_, y_trn_), chr(37))\n",
    "print(\"Test Accuracy:\", 100*rf_clf.score(X_tst_, y_tst_), chr(37))\n",
    "\n",
    "y_tst_pred_ = rf_clf.predict(X_tst_)\n",
    "print(\"====================================================================\")\n",
    "print(\"Confusion matrix: \\n\", confusion_matrix(y_tst_, y_tst_pred_))\n",
    "print(\"====================================================================\")\n",
    "print(\"Classification report: \\n\", classification_report(y_tst_, y_tst_pred_, digits=4))\n",
    "print(\"====================================================================\")\n",
    "\n",
    "\n",
    "# ## 4. LOGISTIC REGRESSION MODEL\n",
    "\n",
    "# C= np.logspace(-4,4,9)\n",
    "# solver=['newton-cg']\n",
    "\n",
    "# param_grid = [{'solver': solver, 'C': C}]\n",
    "# lr_grid_search = GridSearchCV(LogisticRegression(max_iter=100),\n",
    "#                            param_grid=param_grid, cv=3, scoring='f1_weighted',\n",
    "#                            n_jobs=-1, return_train_score=True, verbose=1)\n",
    "# lr_grid_search.fit(X_trn_scaled_, y_trn_)\n",
    "# lr_clf = lr_grid_search.best_estimator_\n",
    "# print(lr_clf)\n",
    "\n",
    "# print(\"Train Accuracy:\", 100*lr_clf.score(X_trn_scaled_, y_trn_), chr(37))\n",
    "# print(\"Test Accuracy:\", 100*lr_clf.score(X_tst_scaled_, y_tst_), chr(37))\n",
    "\n",
    "## 5. ADABOOST MODEL\n",
    "\n",
    "print(\"ADABOOST MODEL:\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators' : [100, 200, 300],\n",
    "    'learning_rate' : [0.001, 0.01, 0.1]\n",
    "}\n",
    "ad_grid_search = GridSearchCV(AdaBoostClassifier(), param_grid = param_grid,\n",
    "                                cv=3, scoring='f1_weighted', n_jobs=-1, return_train_score=True, verbose=1\n",
    "                                )\n",
    "ad_grid_search.fit(X_trn_, y_trn_)\n",
    "\n",
    "ad_clf = ad_grid_search.best_estimator_\n",
    "\n",
    "print(ad_clf)\n",
    "\n",
    "print(\"Train Accuracy:\", 100*ad_clf.score(X_trn_, y_trn_), chr(37))\n",
    "print(\"Test Accuracy:\", 100*ad_clf.score(X_tst_, y_tst_), chr(37))\n",
    "\n",
    "y_tst_pred_ = ad_clf.predict(X_tst_)\n",
    "print(\"====================================================================\")\n",
    "print(\"Confusion matrix: \\n\", confusion_matrix(y_tst_, y_tst_pred_))\n",
    "print(\"====================================================================\")\n",
    "print(\"Classification report: \\n\", classification_report(y_tst_, y_tst_pred_, digits=4))\n",
    "print(\"====================================================================\")\n",
    "\n",
    "\n",
    "# ## 6. GAUSIAN NAIVE BAYSESSIAN\n",
    "\n",
    "# params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "# gnb_grid_search = GridSearchCV(GaussianNB(), param_grid=params_NB, cv=3, scoring='accuracy',return_train_score=True)\n",
    "# gnb_grid_search.fit(X_trn_scaled_, y_trn_)\n",
    "\n",
    "# gnb_clf = gnb_grid_search.best_estimator_\n",
    "\n",
    "# print(gnb_clf)\n",
    "\n",
    "# print(\"Train Accuracy:\", 100*gnb_clf.score(X_trn_scaled_, y_trn_), chr(37))\n",
    "# print(\"Test Accuracy:\", 100*gnb_clf.score(X_tst_scaled_, y_tst_), chr(37))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 - GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
